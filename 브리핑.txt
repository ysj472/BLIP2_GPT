BLIP2 + GPT-4-turbo,
 조합

구조:
BLIP2 → 이미지를 자연어로 설명 (로컬에서 처리 가능)

GPT-4-turbo (OpenAI) → 질문 + 설명으로 답변 생성


실제 구성 흐름

1. 사용자: 이미지 + 질문 업로드
2. 서버: 이미지 -> BLIP2 -> 설명 생성
3. 설명 + 질문 -> GPT-4-turbo -> 답변 생성
4. 사용자: 자연어로 된 결과 응답

필요한 요소
BLIP2: Huggingface Transformers (Salesforce/blip-image-captioning-base)
GPT-4-turbo API: OpenAI API 키 필요 (프리티어 월 ~$5 한도 있음)
FastAPI: 서버 구성 (질문 + 이미지 수신)
uvicorn: FastAPI 실행용

FastAPI란?
python으로 만든 웹 api 서버 프레임워크.(백엔드 서버를 만드는 도구)
사용자로부터 이미지 + 질문 받기 -> /ask-image라는 api 엔드포인트를 만들어둠
이미지 저장 & 분석 -> BLIP2로 이미지 설명 생성
GPT에게 요청 보내기 -> OpenAI API에 질문 + 이미지 설명 전달
결과 응답 반환 -> 사용자에게 JSON 형식으로 응답

사용자와 GPT를 연결해주는 파이프라인 역할

BLIP: 이미지를 보고 사람처럼 설명을 생성해주는 AI 모델
텍스트 캡션 생성, 이미지 질문 응답, 멀티모달 이해에 특화됨.

BLIP2는 PIL 이미지 객체를 받아서(pillow 라이브러리: 내부적으로 이미지를 메모리에서 픽셀 값으로 처리->바이너리 변환 필요 x)
내부 모델로 시각적 특징을 추출하고
자연어 설명을 생성.

Pillow가 이미지 파일을 열 때 내부적으로 알아서 바이너리 데이터를 읽고 픽셀 단위로 변환하는 구조.

바이너리나 base64 변환은 필요하지 않음. (그건 GPT Vision 전용입니다)

explanation.py: BLIP2 모델을 사용하여 이미지에 대한 설명을 생성
**"Bootstrapping Language-Image Pretraining"**이라는 모델로, 이미지와 텍스트 간의 관계를 학습하여 이미지를 자연어로 설명하는 데 사용

**BlipProcessor**와 BlipForConditionalGeneration: HuggingFace Transformers 라이브러리에서 BLIP2 모델을 사용하기 위한 클래스를 불러옴
PIL: 이미지를 열고 처리하는 데 필요한 라이브러리
torch: PyTorch 라이브러리, BLIP2는 PyTorch로 학습된 모델

@app.post("/ask-image")